#!/usr/bin/env python
import logging
import requests

from lxml import html

from gather import Gatherer, daterange, create_urls

class RGatherer(Gatherer):
    def __init__(self, startDate, endDate, storage_config, config=None, dlPath="./articles", numThreads=8):
        Gatherer.__init__(self, startDate, endDate, storage_config, config, dlPath, numThreads)
        if config is None:
            raise Exception("Config cannot be none")
        self.url_template = config["url_template"]

    def find_all_articles(self):
        for url in create_urls(self.url_template, daterange(self.startDate, self.endDate)):
            for article_url in self.url_to_articles(url):
                self.queue.put(article_url)

    def url_to_articles(self, url):
        """ the linkes generated by the template are per day and need to be broken down """
        page = requests.get(url)
        tree = html.fromstring(page.content)
        return tree.xpath('//div[@class="headlineMed"]/a/@href')


